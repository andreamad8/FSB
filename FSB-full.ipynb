{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import load_model\n",
    "from prompts.generic_prompt import load_prefix, generate_response_interactive, select_prompt_interactive\n",
    "from prompts.generic_prompt_parser import load_prefix as load_prefix_parse\n",
    "from prompts.persona_chat import convert_sample_to_shot_persona\n",
    "from prompts.persona_chat_memory import convert_sample_to_shot_msc, convert_sample_to_shot_msc_interact\n",
    "from prompts.emphatetic_dialogue import convert_sample_to_shot_ed\n",
    "from prompts.daily_dialogue import convert_sample_to_shot_DD_prefix, convert_sample_to_shot_DD_inference\n",
    "from prompts.persona_parser import convert_sample_to_shot_msc as convert_sample_to_shot_msc_parse\n",
    "from prompts.wizard_of_wikipedia import convert_sample_to_shot_wow, convert_sample_to_shot_wow_interact\n",
    "from prompts.wizard_of_wikipedia_parse import convert_sample_to_shot_wow as convert_sample_to_shot_wow_parse\n",
    "from prompts.wizard_of_internet import convert_sample_to_shot_wit, convert_sample_to_shot_wit_interact\n",
    "from prompts.wizard_of_internet_parser import convert_sample_to_shot_wit as convert_sample_to_shot_wit_parse\n",
    "from prompts.dialKG import convert_sample_to_shot_dialKG, convert_sample_to_shot_dialKG_interact\n",
    "from prompts.dialKG_parser import convert_sample_to_shot_dialKG as convert_sample_to_shot_dialKG_parse\n",
    "from prompts.skill_selector import convert_sample_to_shot_selector\n",
    "from utils.wit_parlai_retriever import SearchEngineRetriever\n",
    "import wikipedia\n",
    "import random\n",
    "import torch\n",
    "import pprint\n",
    "from nltk.tokenize import sent_tokenize\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "args = type('', (), {})()\n",
    "args.multigpu = False\n",
    "device = 6\n",
    "safety_level = 6\n",
    "shot_selector = 6\n",
    "sample_skill = False\n",
    "ks = SearchEngineRetriever(search_server=\"ADDRESS:8081\")\n",
    "## To use GPT-Jumbo (178B) set this to true and input your api-key\n",
    "## Visit https://studio.ai21.com/account for more info\n",
    "## AI21 provides 10K tokens per day, so you can try only for few turns\n",
    "api = False\n",
    "api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "LOADING EleutherAI/gpt-neo-1.3B\nDONE LOADING\n"
    }
   ],
   "source": [
    "## Load LM and tokenizer\n",
    "## You can try different LMs: \n",
    "##   gpt2 \n",
    "##   gpt2-medium \n",
    "##   gpt2-large\n",
    "##   gpt2-xl\n",
    "##   EleutherAI/gpt-neo-1.3B\n",
    "##   EleutherAI/gpt-neo-2.7B\n",
    "##   EleutherAI/gpt-j-6B\n",
    "## So far the largest I could load is gpt2-large\n",
    "model_checkpoint = \"EleutherAI/gpt-neo-1.3B\"\n",
    "model, tokenizer, max_seq = load_model(args,model_checkpoint,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the search engine retriever\n",
    "# cd retrievers/ParlAI_SearchEngine\n",
    "# python search_server.py serve --host 0.0.0.0:8081\n",
    "# python search_server.py serve --host 0.0.0.0:8081 --search_engine=\"Bing\" --use_description_only --subscription_key \"API_KEY\" --strip_html_menus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the config dictionary used to select the template converter\n",
    "mapper = {\n",
    "          \"persona\": {\"shot_converter\":convert_sample_to_shot_persona, \n",
    "                    \"shot_converter_inference\": convert_sample_to_shot_persona,\n",
    "                     \"file_data\":\"data/persona/\",\"with_knowledge\":None,\n",
    "                     \"shots\":{1024:[0,1,2],2048:[0,1,2,3,4,5]},\"max_shot\":{1024:2,2048:3},\n",
    "                     \"shot_separator\":\"\\n\\n\",\n",
    "                     \"meta_type\":\"all\",\"gen_len\":50,\"max_number_turns\":5},\n",
    "          \"msc\": {\"shot_converter\":convert_sample_to_shot_msc, \n",
    "                    \"shot_converter_inference\": convert_sample_to_shot_msc_interact,\n",
    "                     \"file_data\":\"data/msc/session-2-\",\"with_knowledge\":None,\n",
    "                     \"shots\":{1024:[0,1],2048:[0,1,3]},\"max_shot\":{1024:1,2048:3},\n",
    "                     \"shot_separator\":\"\\n\\n\",\n",
    "                     \"meta_type\":\"all\",\"gen_len\":50,\"max_number_turns\":3},\n",
    "          \"ed\": {\"shot_converter\":convert_sample_to_shot_ed, \n",
    "                 \"shot_converter_inference\": convert_sample_to_shot_ed,\n",
    "                 \"file_data\":\"data/ed/\",\"with_knowledge\":None,\n",
    "                  \"shots\":{1024:[0,1,7],2048:[0,1,17]},\"max_shot\":{1024:7,2048:17},\n",
    "                  \"shot_separator\":\"\\n\\n\",\n",
    "                  \"meta_type\":\"none\",\"gen_len\":50,\"max_number_turns\":5},\n",
    "          \"DD\": {\"shot_converter\":convert_sample_to_shot_DD_prefix, \n",
    "                 \"shot_converter_inference\": convert_sample_to_shot_DD_inference,\n",
    "                 \"file_data\":\"data/dailydialog/\",\"with_knowledge\":False,\n",
    "                  \"shots\":{1024:[0,1,2],2048:[0,1,6]},\"max_shot\":{1024:2,2048:6},\n",
    "                  \"shot_separator\":\"\\n\\n\",\n",
    "                  \"meta_type\":\"all_turns\",\"gen_len\":50,\"max_number_turns\":5},\n",
    "          \"wow\": {\"shot_converter\":convert_sample_to_shot_wow, \n",
    "                 \"shot_converter_inference\": convert_sample_to_shot_wow_interact,\n",
    "                 \"file_data\":\"data/wow/\",\"with_knowledge\":True,\n",
    "                  \"shots\":{1024:[0,1,2],2048:[4,3,2,1,0]},\"max_shot\":{1024:1,2048:3},\n",
    "                  \"shot_separator\":\"\\n\\n\",\n",
    "                  \"meta_type\":\"incremental\",\"gen_len\":60,\"max_number_turns\":5},\n",
    "          \"wit\": {\"shot_converter\":convert_sample_to_shot_wit, \n",
    "                 \"shot_converter_inference\": convert_sample_to_shot_wit_interact,\n",
    "                 \"file_data\":\"data/wit/\",\"with_knowledge\":True,\n",
    "                  \"shots\":{1024:[0,1],2048:[0,1,2,3]},\"max_shot\":{1024:1,2048:3},\n",
    "                  \"shot_separator\":\"\\n\\n\",\n",
    "                  \"meta_type\":\"incremental\",\"gen_len\":60,\"max_number_turns\":4},\n",
    "       #    \"dialKG\": {\"shot_converter\":convert_sample_to_shot_dialKG, \n",
    "       #           \"shot_converter_inference\": convert_sample_to_shot_dialKG_interact,\n",
    "       #           \"file_data\":\"data/dialKG/\",\"with_knowledge\":True,\n",
    "       #            \"shots\":{1024:[0,1,3],2048:[0,1,9]},\"max_shot\":{1024:3,2048:1},\n",
    "       #            \"shot_separator\":\"\\n\\n\",\n",
    "       #            \"meta_type\":\"incremental\",\"gen_len\":50,\"max_number_turns\":4},\n",
    "          \"wow-parse\": {\"shot_converter\":convert_sample_to_shot_wow_parse, \n",
    "                 \"file_data\":\"data/wow/parse-\",\"level\":\"dialogue\", \"retriever\":\"wiki\",\n",
    "                  \"shots\":{1024:[0, 1, 5],2048:[0, 1, 5, 10]},\"max_shot\":{1024:5,2048:10},\n",
    "                  \"shot_separator\":\"\\n\\n\", \"meta_type\":\"last_turn\",\"gen_len\":50,\"max_number_turns\":2},\n",
    "          \"wit-parse\": {\"shot_converter\":convert_sample_to_shot_wit_parse, \n",
    "                 \"file_data\":\"data/wit/\",\"level\":\"dialogue\",\"max_shot\":{1024:1,2048:4},\n",
    "                  \"shots\":{1024:[0,1],2048:[0, 1, 2, 3, 4]},\"shot_separator\":\"\\n\\n\", \"retriever\":\"internet\",\n",
    "                  \"meta_type\":\"query\",\"gen_len\":50,\"max_number_turns\":2},\n",
    "       #    \"dialKG-parse\": {\"shot_converter\":convert_sample_to_shot_dialKG_parse, \n",
    "       #           \"file_data\":\"data/dialKG/\",\"level\":\"dialogue\", \"max_shot\":{1024:3,2048:9},\n",
    "       #            \"shots\":{1024:[0,1,2,3],2048:[0, 1, 5, 9]},\"shot_separator\":\"\\n\\n\", \"retriever\":\"graph\",\n",
    "       #            \"meta_type\":\"incremental\",\"gen_len\":50,\"max_number_turns\":5},\n",
    "          \"msc-parse\": {\"shot_converter\":convert_sample_to_shot_msc_parse, \"max_shot\":{1024:1,2048:2},\n",
    "                 \"file_data\":\"data/msc/parse-session-1-\",\"level\":\"dialogue\", \"retriever\":\"none\",\n",
    "                  \"shots\":{1024:[0,1],2048:[0, 1, 2]},\"shot_separator\":\"\\n\\n\",\n",
    "                  \"meta_type\":\"incremental\",\"gen_len\":50,\"max_number_turns\":3},\n",
    "               \n",
    "         }\n",
    "## This is the config dictionary used to select the template converter\n",
    "mapper_safety = {\n",
    "          \"safety_topic\": {\"file_data\":\"data/safety_layers/safety_topic.json\",\"with_knowledge\":None,\n",
    "                     \"shots\":{1024:[0,1,2],2048:[0,1,2,3,4,5]},\"max_shot\":{1024:2,2048:3},\n",
    "                     \"shot_separator\":\"\\n\\n\",\n",
    "                     \"meta_type\":\"all\",\"gen_len\":50,\"max_number_turns\":2},\n",
    "          \"safety_nonadv\": {\"file_data\":\"data/safety_layers/safety_nonadv.json\",\"with_knowledge\":None,\n",
    "                     \"shots\":{1024:[0,1,2],2048:[0,1,2,3,4,5]},\"max_shot\":{1024:2,2048:3},\n",
    "                     \"shot_separator\":\"\\n\\n\",\n",
    "                     \"meta_type\":\"all\",\"gen_len\":50,\"max_number_turns\":2},\n",
    "       #    \"safety_adv\": {\"file_data\":\"data/safety_layers/safety_adv.json\",\"with_knowledge\":None,\n",
    "       #               \"shots\":{1024:[0,1,2],2048:[0,1,2,3,4,5]},\"max_shot\":{1024:2,2048:3},\n",
    "       #               \"shot_separator\":\"\\n\\n\",\n",
    "       #               \"meta_type\":\"all\",\"gen_len\":50,\"max_number_turns\":2},\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loaded persona dict_keys([6]) shots for shuffle 0!\nLoaded persona dict_keys([0, 1, 2, 3, 4, 5]) shots for shuffle 0!\nLoaded msc dict_keys([6]) shots for shuffle 0!\nLoaded msc dict_keys([0, 1, 3]) shots for shuffle 0!\nLoaded ed dict_keys([6]) shots for shuffle 0!\nLoaded ed dict_keys([0, 1, 17]) shots for shuffle 0!\nLoaded DD dict_keys([6]) shots for shuffle 0!\nLoaded DD dict_keys([0, 1, 6]) shots for shuffle 0!\nLoaded wow dict_keys([6]) shots for shuffle 0!\nLoaded wow dict_keys([4, 3, 2, 1, 0]) shots for shuffle 0!\nLoaded wit dict_keys([6]) shots for shuffle 0!\nLoaded wit dict_keys([0, 1, 2, 3]) shots for shuffle 0!\nLoaded wow-parse dict_keys([0, 1, 5, 10]) shots for shuffle 0!\nLoaded wit-parse dict_keys([0, 1, 2, 3, 4]) shots for shuffle 0!\nLoaded msc-parse dict_keys([0, 1, 2]) shots for shuffle 0!\nLoaded safety_topic dict_keys([6]) shots for shuffle 0!\nLoaded safety_nonadv dict_keys([6]) shots for shuffle 0!\n"
    }
   ],
   "source": [
    "available_datasets = mapper.keys()\n",
    "prompt_dict = {}\n",
    "prompt_parse = {}\n",
    "prompt_skill_selector = {}\n",
    "for d in available_datasets:\n",
    "    if \"parse\" in d:\n",
    "        prompt_parse[d] = load_prefix_parse(tokenizer=tokenizer, shots_value=mapper[d][\"shots\"][max_seq], \n",
    "                                shot_converter=mapper[d][\"shot_converter\"], \n",
    "                                file_shot=mapper[d][\"file_data\"]+\"valid.json\", \n",
    "                                name_dataset=d, level=mapper[d][\"level\"], \n",
    "                                shot_separator=mapper[d][\"shot_separator\"],sample_times=1)[0]\n",
    "    else:\n",
    "        prompt_skill_selector[d] = load_prefix(tokenizer=tokenizer, shots_value=[shot_selector], \n",
    "                    shot_converter=convert_sample_to_shot_selector, \n",
    "                    file_shot= mapper[d][\"file_data\"]+\"train.json\" if \"smd\" in d else mapper[d][\"file_data\"]+\"valid.json\", \n",
    "                    name_dataset=d, with_knowledge=None, \n",
    "                    shot_separator=mapper[d][\"shot_separator\"],sample_times=1)[0]\n",
    "        prompt_dict[d] = load_prefix(tokenizer=tokenizer, shots_value=mapper[d][\"shots\"][max_seq], \n",
    "                    shot_converter=mapper[d][\"shot_converter\"], \n",
    "                    file_shot=mapper[d][\"file_data\"]+\"valid.json\", \n",
    "                    name_dataset=d, with_knowledge=mapper[d][\"with_knowledge\"], \n",
    "                    shot_separator=mapper[d][\"shot_separator\"],sample_times=1)[0]\n",
    "    \n",
    "## add safety prompts\n",
    "for d in mapper_safety.keys():\n",
    "    prompt_skill_selector[d] = load_prefix(tokenizer=tokenizer, shots_value=[safety_level], \n",
    "            shot_converter=convert_sample_to_shot_selector, \n",
    "            file_shot= mapper_safety[d][\"file_data\"], \n",
    "            name_dataset=d, with_knowledge=None, \n",
    "            shot_separator=mapper_safety[d][\"shot_separator\"],sample_times=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parsers(args, model, tokenizer, device, max_seq, dialogue, skill, prefix_dict):\n",
    "\n",
    "    if skill not in [\"msc\", \"wow\", \"wit\"]: return dialogue\n",
    "\n",
    "    ### parse \n",
    "    d_p = f\"{skill}-parse\"\n",
    "    print(f\"Parse with {d_p}\")\n",
    "\n",
    "    prefix = prefix_dict[d_p].get(mapper[d_p][\"max_shot\"][max_seq])\n",
    "    query = generate_response_interactive(model, tokenizer, shot_converter=mapper[d_p][\"shot_converter\"], \n",
    "                                                dialogue=dialogue, prefix=prefix, \n",
    "                                                device=device,  with_knowledge=None, \n",
    "                                                meta_type=None, gen_len=50, \n",
    "                                                beam=1, max_seq=max_seq, eos_token_id=198, \n",
    "                                                do_sample=False, multigpu=False, api=api, api_key=api_key)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    if query.lower() == \"none\": return dialogue\n",
    "\n",
    "    if skill == \"wow\" and query not in dialogue[\"query_mem\"]:\n",
    "        dialogue[\"query_mem\"].append(query)\n",
    "        try:\n",
    "            retrieve_K = wikipedia.summary(query, sentences=1)\n",
    "        except:\n",
    "            retrieve_K = \"None\"\n",
    "        dialogue[\"KB_wiki\"][-1] = [retrieve_K]\n",
    "    elif skill == \"wit\" and query not in dialogue[\"query_mem\"]:\n",
    "        dialogue[\"query_mem\"].append(query)\n",
    "        try: \n",
    "            page = ks.retrieve(queries=[query], num_ret=1)[0]\n",
    "            retrieve_K = sent_tokenize(page[0]['content'])[1] \n",
    "        except:\n",
    "            retrieve_K = \"None\"\n",
    "        dialogue[\"KB_internet\"][-1] = [retrieve_K]\n",
    "        dialogue[\"query\"][-1] = [query]\n",
    "    elif skill == \"dialKG\":\n",
    "        dialogue[\"KG\"][-1] = [retrieve_K]\n",
    "    elif skill == \"msc\":\n",
    "        dialogue[\"user\"].append(query)\n",
    "        dialogue[\"user_memory\"][-1] = [query]\n",
    "    return dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Parse with msc-parse\nQuery: None\nFSB (msc) >>> I think AI will be the future of technology.\nFSB (DD) >>> No , I don't think so .\nFSB (ed) >>> No , I don't .\nFSB (persona) >>> Yes , I do .\nParse with wit-parse\nQuery: None\nFSB (wit) >>> No , I don't .\nParse with msc-parse\n"
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'completions'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5964/2916410422.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         dialogue = run_parsers(args, model, tokenizer, device=device, max_seq=max_seq,\n\u001b[1;32m     36\u001b[0m                                 \u001b[0mdialogue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdialogue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskill\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                                 prefix_dict=prompt_parse)\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m## generate response based on skills\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mskill\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mskill\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_shot\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5964/1701453542.py\u001b[0m in \u001b[0;36mrun_parsers\u001b[0;34m(args, model, tokenizer, device, max_seq, dialogue, skill, prefix_dict)\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                 \u001b[0mmeta_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                 \u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m198\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                                 do_sample=False, multigpu=False, api=api, api_key=api_key)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Query: {query}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FSB/prompts/generic_prompt.py\u001b[0m in \u001b[0;36mgenerate_response_interactive\u001b[0;34m(model, tokenizer, shot_converter, dialogue, prefix, device, with_knowledge, meta_type, gen_len, beam, max_seq, eos_token_id, do_sample, multigpu, api, api_key)\u001b[0m\n\u001b[1;32m    462\u001b[0m         )\n\u001b[1;32m    463\u001b[0m         \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'completions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'completions'"
     ]
    }
   ],
   "source": [
    "# try: \n",
    "max_number_turns = 5\n",
    "dialogue = {\"dialogue\":[],\"meta\":[],\"user\":[],\n",
    "            \"assistant\":[],\"user_memory\":[], \n",
    "            \"KG\":[], \"KB_internet\": [], \n",
    "            \"KB_wiki\": [], \"query\":[],\n",
    "            \"query_mem\":[]}\n",
    "## This meta information is the persona of the FSB\n",
    "dialogue[\"meta\"] = dialogue[\"assistant\"] = [\n",
    "                \"i am the smartest chat-bot around .\",\n",
    "                \"my name is FSB . \",\n",
    "                \"i love chatting with people .\",\n",
    "                \"my creator is Andrea\"\n",
    "                ]\n",
    "t = 10\n",
    "while t>0: \n",
    "    t -= 1\n",
    "    user_utt = input(\">>> \")\n",
    "    dialogue[\"dialogue\"].append([user_utt,\"\"])\n",
    "    ## run the skill selector\n",
    "    skill = select_prompt_interactive(model, tokenizer, \n",
    "                                    shot_converter=convert_sample_to_shot_selector, \n",
    "                                    dialogue=dialogue, prompt_dict=prompt_skill_selector, \n",
    "                                    device=device, max_seq=max_seq, max_shot=shot_selector, sample=sample_skill)\n",
    "    dialogue[\"user_memory\"].append([])\n",
    "    dialogue[\"KB_wiki\"].append([])\n",
    "    dialogue[\"KB_internet\"].append([])\n",
    "    dialogue[\"query\"].append([])\n",
    "    dialogue[\"KG\"].append([])\n",
    "\n",
    "    if \"safety\" in skill: \n",
    "        response = \"Shall we talk about something else?\"\n",
    "    else:\n",
    "        ## parse user dialogue history ==> msc-parse\n",
    "        dialogue = run_parsers(args, model, tokenizer, device=device, max_seq=max_seq,\n",
    "                                dialogue=dialogue, skill=skill,  \n",
    "                                prefix_dict=prompt_parse)\n",
    "        ## generate response based on skills\n",
    "        prefix = prompt_dict[skill].get(mapper[skill][\"max_shot\"][max_seq])\n",
    "        response = generate_response_interactive(model, tokenizer, shot_converter=mapper[skill][\"shot_converter_inference\"], \n",
    "                                                    dialogue=dialogue, prefix=prefix, \n",
    "                                                    device=device, with_knowledge=mapper[skill][\"with_knowledge\"], \n",
    "                                                    meta_type=mapper[skill][\"meta_type\"], gen_len=50, \n",
    "                                                    beam=1, max_seq=max_seq, eos_token_id=198, \n",
    "                                                    do_sample=True, multigpu=False, api=api, api_key=api_key)\n",
    "                    \n",
    "\n",
    "    print(f\"FSB ({skill}) >>> {response}\")\n",
    "    dialogue[\"dialogue\"][-1][1] = response\n",
    "    dialogue[\"dialogue\"] = dialogue[\"dialogue\"][-max_number_turns:]\n",
    "    dialogue[\"user_memory\"] = dialogue[\"user_memory\"][-max_number_turns:]\n",
    "    dialogue[\"KB_wiki\"] = dialogue[\"KB_wiki\"][-max_number_turns:]\n",
    "    dialogue[\"KB_internet\"] = dialogue[\"KB_internet\"][-max_number_turns:]\n",
    "    dialogue[\"query\"] = dialogue[\"query\"][-max_number_turns:]\n",
    "    dialogue[\"KG\"] = dialogue[\"KG\"][-max_number_turns:]\n",
    "print(\"\\n\\nThis is the conversation history with its meta-data!\\n\\n\")\n",
    "print(pp.pprint(dialogue))\n",
    "# except: \n",
    "#     print(\"\\n\\n An error has occured: this is the conversation history with its meta-data!\\n\\n\")\n",
    "#     print(pp.pprint(dialogue))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python371064bit2821c06c19f04f5b9ccde7ca4af69bcf",
   "display_name": "Python 3.7.10 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}