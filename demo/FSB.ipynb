{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import load_model\n",
    "from prompts.generic_prompt import load_prefix, generate_response_interactive, select_prompt_interactive\n",
    "from prompts.generic_prompt_parser import load_prefix as load_prefix_parse\n",
    "from prompts.persona_chat import convert_sample_to_shot_persona\n",
    "from prompts.persona_chat_memory import convert_sample_to_shot_msc, convert_sample_to_shot_msc_interact\n",
    "from prompts.persona_parser import convert_sample_to_shot_msc as convert_sample_to_shot_msc_parse\n",
    "from prompts.emphatetic_dialogue import convert_sample_to_shot_ed\n",
    "from prompts.daily_dialogue import convert_sample_to_shot_DD_prefix, convert_sample_to_shot_DD_inference\n",
    "from prompts.skill_selector import convert_sample_to_shot_selector\n",
    "import random\n",
    "import torch\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "args = type('', (), {})()\n",
    "args.multigpu = False\n",
    "device = 4\n",
    "\n",
    "## To use GPT-Jumbo (178B) set this to true and input your api-key\n",
    "## Visit https://studio.ai21.com/account for more info\n",
    "## AI21 provides 10K tokens per day, so you can try only for few turns\n",
    "api = False\n",
    "api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the config dictionary used to select the template converter\n",
    "mapper = {\n",
    "          \"persona\": {\"shot_converter\":convert_sample_to_shot_persona, \n",
    "                    \"shot_converter_inference\": convert_sample_to_shot_persona,\n",
    "                     \"file_data\":\"data/persona/\",\"with_knowledge\":None,\n",
    "                     \"shots\":{1024:[0,1,2],2048:[0,1,2,3,4,5]},\"max_shot\":{1024:2,2048:3},\n",
    "                     \"shot_separator\":\"\\n\\n\",\n",
    "                     \"meta_type\":\"all\",\"gen_len\":50,\"max_number_turns\":5},\n",
    "          \"msc\": {\"shot_converter\":convert_sample_to_shot_msc, \n",
    "                    \"shot_converter_inference\": convert_sample_to_shot_msc_interact,\n",
    "                     \"file_data\":\"data/msc/session-2-\",\"with_knowledge\":None,\n",
    "                     \"shots\":{1024:[0,1],2048:[0,1,3]},\"max_shot\":{1024:1,2048:3},\n",
    "                     \"shot_separator\":\"\\n\\n\",\n",
    "                     \"meta_type\":\"all\",\"gen_len\":50,\"max_number_turns\":3},\n",
    "          \"ed\": {\"shot_converter\":convert_sample_to_shot_ed, \n",
    "                 \"shot_converter_inference\": convert_sample_to_shot_ed,\n",
    "                 \"file_data\":\"data/ed/\",\"with_knowledge\":None,\n",
    "                  \"shots\":{1024:[0,1,7],2048:[0,1,17]},\"max_shot\":{1024:7,2048:17},\n",
    "                  \"shot_separator\":\"\\n\\n\",\n",
    "                  \"meta_type\":\"none\",\"gen_len\":50,\"max_number_turns\":5},\n",
    "          \"DD\": {\"shot_converter\":convert_sample_to_shot_DD_prefix, \n",
    "                 \"shot_converter_inference\": convert_sample_to_shot_DD_inference,\n",
    "                 \"file_data\":\"data/dailydialog/\",\"with_knowledge\":False,\n",
    "                  \"shots\":{1024:[0,1,2],2048:[0,1,6]},\"max_shot\":{1024:2,2048:6},\n",
    "                  \"shot_separator\":\"\\n\\n\",\n",
    "                  \"meta_type\":\"all_turns\",\"gen_len\":50,\"max_number_turns\":5},\n",
    "          \"msc-parse\": {\"shot_converter\":convert_sample_to_shot_msc_parse, \"max_shot\":{1024:1,2048:2},\n",
    "                 \"file_data\":\"data/msc/parse-session-1-\",\"level\":\"dialogue\", \"retriever\":\"none\",\n",
    "                  \"shots\":{1024:[0,1],2048:[0, 1, 2]},\"shot_separator\":\"\\n\\n\",\n",
    "                  \"meta_type\":\"incremental\",\"gen_len\":50,\"max_number_turns\":3},\n",
    "                  \n",
    "         }\n",
    "## This is the config dictionary used to select the template converter\n",
    "mapper_safety = {\n",
    "          \"safety_topic\": {\"file_data\":\"data/safety_layers/safety_topic.json\",\"with_knowledge\":None,\n",
    "                     \"shots\":{1024:[0,1,2],2048:[0,1,2,3,4,5]},\"max_shot\":{1024:2,2048:3},\n",
    "                     \"shot_separator\":\"\\n\\n\",\n",
    "                     \"meta_type\":\"all\",\"gen_len\":50,\"max_number_turns\":2},\n",
    "          \"safety_nonadv\": {\"file_data\":\"data/safety_layers/safety_nonadv.json\",\"with_knowledge\":None,\n",
    "                     \"shots\":{1024:[0,1,2],2048:[0,1,2,3,4,5]},\"max_shot\":{1024:2,2048:3},\n",
    "                     \"shot_separator\":\"\\n\\n\",\n",
    "                     \"meta_type\":\"all\",\"gen_len\":50,\"max_number_turns\":2},\n",
    "          \"safety_adv\": {\"file_data\":\"data/safety_layers/safety_adv.json\",\"with_knowledge\":None,\n",
    "                     \"shots\":{1024:[0,1,2],2048:[0,1,2,3,4,5]},\"max_shot\":{1024:2,2048:3},\n",
    "                     \"shot_separator\":\"\\n\\n\",\n",
    "                     \"meta_type\":\"all\",\"gen_len\":50,\"max_number_turns\":2},\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "LOADING EleutherAI/gpt-neo-1.3B\nDONE LOADING\n"
    }
   ],
   "source": [
    "## Load LM and tokenizer\n",
    "## You can try different LMs: \n",
    "##   gpt2 \n",
    "##   gpt2-medium \n",
    "##   gpt2-large\n",
    "##   gpt2-xl\n",
    "##   EleutherAI/gpt-neo-1.3B\n",
    "##   EleutherAI/gpt-neo-2.7B\n",
    "##   EleutherAI/gpt-j-6B\n",
    "## So far the largest I could load is gpt2-large\n",
    "model_checkpoint = \"EleutherAI/gpt-neo-1.3B\"\n",
    "model, tokenizer, max_seq = load_model(args,model_checkpoint,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loaded persona dict_keys([6]) shots for shuffle 0!\nLoaded persona dict_keys([0, 1, 2, 3, 4, 5]) shots for shuffle 0!\nLoaded msc dict_keys([6]) shots for shuffle 0!\nLoaded msc dict_keys([0, 1, 3]) shots for shuffle 0!\nLoaded ed dict_keys([6]) shots for shuffle 0!\nLoaded ed dict_keys([0, 1, 17]) shots for shuffle 0!\nLoaded DD dict_keys([6]) shots for shuffle 0!\nLoaded DD dict_keys([0, 1, 6]) shots for shuffle 0!\nLoaded msc-parse dict_keys([0, 1, 2]) shots for shuffle 0!\nLoaded safety_topic dict_keys([6]) shots for shuffle 0!\nLoaded safety_nonadv dict_keys([6]) shots for shuffle 0!\nLoaded safety_adv dict_keys([6]) shots for shuffle 0!\n"
    }
   ],
   "source": [
    "available_datasets = mapper.keys()\n",
    "prompt_dict = {}\n",
    "prompt_parse = {}\n",
    "prompt_skill_selector = {}\n",
    "for d in available_datasets:\n",
    "    if \"parse\" in d:\n",
    "        prompt_parse[d] = load_prefix_parse(tokenizer=tokenizer, shots_value=mapper[d][\"shots\"][max_seq], \n",
    "                                shot_converter=mapper[d][\"shot_converter\"], \n",
    "                                file_shot=mapper[d][\"file_data\"]+\"valid.json\", \n",
    "                                name_dataset=d, level=mapper[d][\"level\"], \n",
    "                                shot_separator=mapper[d][\"shot_separator\"],sample_times=1)[0]\n",
    "    else:\n",
    "        prompt_skill_selector[d] = load_prefix(tokenizer=tokenizer, shots_value=[6], \n",
    "                    shot_converter=convert_sample_to_shot_selector, \n",
    "                    file_shot= mapper[d][\"file_data\"]+\"train.json\" if \"smd\" in d else mapper[d][\"file_data\"]+\"valid.json\", \n",
    "                    name_dataset=d, with_knowledge=None, \n",
    "                    shot_separator=mapper[d][\"shot_separator\"],sample_times=1)[0]\n",
    "        prompt_dict[d] = load_prefix(tokenizer=tokenizer, shots_value=mapper[d][\"shots\"][max_seq], \n",
    "                    shot_converter=mapper[d][\"shot_converter\"], \n",
    "                    file_shot=mapper[d][\"file_data\"]+\"valid.json\", \n",
    "                    name_dataset=d, with_knowledge=mapper[d][\"with_knowledge\"], \n",
    "                    shot_separator=mapper[d][\"shot_separator\"],sample_times=1)[0]\n",
    "    \n",
    "## add safety prompts\n",
    "for d in mapper_safety.keys():\n",
    "    prompt_skill_selector[d] = load_prefix(tokenizer=tokenizer, shots_value=[6], \n",
    "            shot_converter=convert_sample_to_shot_selector, \n",
    "            file_shot= mapper_safety[d][\"file_data\"], \n",
    "            name_dataset=d, with_knowledge=None, \n",
    "            shot_separator=mapper_safety[d][\"shot_separator\"],sample_times=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parsers(args, model, tokenizer, device, max_seq, dialogue, skill, prefix_dict):\n",
    "    dialogue[\"user_memory\"].append([])\n",
    "\n",
    "    if skill not in [\"msc\"]: return dialogue\n",
    "\n",
    "    # if d == \"dialKG\":\n",
    "    #     dialogue[\"KG\"].append([])\n",
    "\n",
    "    ### parse \n",
    "    d_p = f\"{skill}-parse\"\n",
    "    # print(f\"Parse with {d_p}\")\n",
    "\n",
    "    prefix = prefix_dict[d_p].get(mapper[d_p][\"max_shot\"][max_seq])\n",
    "    query = generate_response_interactive(model, tokenizer, shot_converter=mapper[d_p][\"shot_converter\"], \n",
    "                                                dialogue=dialogue, prefix=prefix, \n",
    "                                                device=device,  with_knowledge=None, \n",
    "                                                meta_type=None, gen_len=50, \n",
    "                                                beam=1, max_seq=max_seq, eos_token_id=198, \n",
    "                                                do_sample=False, multigpu=False, api=api, api_key=api_key)\n",
    "\n",
    "    # print(f\"Query: {query}\")\n",
    "    # if d == \"wow\":\n",
    "    #     dialogue[\"KB_wiki\"].append([retrieve_K])\n",
    "    # elif d == \"dialKG\":\n",
    "    #     dialogue[\"KG\"][-1] = [retrieve_K]\n",
    "    # elif d == \"wit\":\n",
    "    #     dialogue[\"KB_internet\"].append([retrieve_K])\n",
    "    #     dialogue[\"query\"].append([query])\n",
    "    if skill == \"msc\":\n",
    "        if \"none\" != query:\n",
    "            dialogue[\"user\"].append(query)\n",
    "            dialogue[\"user_memory\"][-1] = [query]\n",
    "    return dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_number_turns = 3\n",
    "dialogue = {\"dialogue\":[],\"meta\":[],\"user\":[],\"assistant\":[],\"user_memory\":[]}\n",
    "## This meta information is the persona of the FSB\n",
    "dialogue[\"meta\"] = dialogue[\"assistant\"] = [\n",
    "                \"i am the smartest chat-bot around .\",\n",
    "                \"my name is FSB . \",\n",
    "                \"i love chatting with people .\",\n",
    "                \"my creator is Andrea\"\n",
    "                ]\n",
    "t = 10\n",
    "while t>0: \n",
    "    t -= 1\n",
    "    user_utt = input(\">>> \")\n",
    "    dialogue[\"dialogue\"].append([user_utt,\"\"])\n",
    "    ## run the skill selector\n",
    "    skill = select_prompt_interactive(model, tokenizer, \n",
    "                                      shot_converter=convert_sample_to_shot_selector, \n",
    "                                      dialogue=dialogue, prompt_dict=prompt_skill_selector, \n",
    "                                      device=device, max_seq=max_seq, max_shot=6)\n",
    "    \n",
    "    if \"safety\" in skill: \n",
    "        response = \"Shall we talk about something else?\"\n",
    "        print(f\"FSB (Safety) >>> {response}\")\n",
    "\n",
    "    else:\n",
    "        ## parse user dialogue history ==> msc-parse\n",
    "        dialogue = run_parsers(args, model, tokenizer, device=device, max_seq=max_seq,\n",
    "                                dialogue=dialogue, skill=skill,  \n",
    "                                prefix_dict=prompt_parse)\n",
    "        ## generate response based on skills\n",
    "        prefix = prompt_dict[skill].get(mapper[skill][\"max_shot\"][max_seq])\n",
    "        response = generate_response_interactive(model, tokenizer, shot_converter=mapper[skill][\"shot_converter_inference\"], \n",
    "                                                    dialogue=dialogue, prefix=prefix, \n",
    "                                                    device=device, with_knowledge=mapper[skill][\"with_knowledge\"], \n",
    "                                                    meta_type=mapper[skill][\"meta_type\"], gen_len=50, \n",
    "                                                    beam=1, max_seq=max_seq, eos_token_id=198, \n",
    "                                                    do_sample=True, multigpu=False, api=api, api_key=api_key)\n",
    "                    \n",
    "\n",
    "        print(f\"FSB ({skill}) >>> {response}\")\n",
    "    dialogue[\"dialogue\"][-1][1] = response\n",
    "    dialogue[\"dialogue\"] = dialogue[\"dialogue\"][-max_number_turns:]\n",
    "    dialogue[\"user_memory\"] = dialogue[\"user_memory\"][-max_number_turns:]\n",
    "print(\"This is the conversation history with its meta-data!\")\n",
    "print(pp.pprint(dialogue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python371064bit2821c06c19f04f5b9ccde7ca4af69bcf",
   "display_name": "Python 3.7.10 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}